# -*- coding: utf-8 -*-
"""Mini-Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11QnQKdeEAn6uLGyGDYiPnYrnqu45BXqN
"""

import numpy as np
import pandas as pd
import matplotlib
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

def get_labeled_features(file_path):
    """Read data from train.csv and split into train and dev sets. Do any
       preprocessing/augmentation steps here and return final features.

    Args:
        file_path (str): path to train.csv

    Returns:
        phi_train, y_train, phi_dev, y_dev
    """
    df = pd.read_csv(file_path)

    msk = np.random.rand(len(df)) < 0.8

    train = df[msk]
    test = df[~msk]

    phi_train=pd.dataframe(train,columns=['type','fixed acidity','volatile acidity','residual sulphur','chlorides','free sulfur','total sulfur','density','pH','sulpahtes','alcohol'])
    y_train=pd.dataframe(train,columns=['quality'])
    phi_dev=pd.dataframe(test,columns=['type','fixed acidity','volatile acidity','residual sulphur','chlorides','free sulfur','total sulfur','density','pH','sulpahtes','alcohol'])
    y_dev=pd.dataframe(test,columns=['quality'])
    phi_train = scaler.fit_transform(phi_train)
    phi_dev = scaler.fit_transform(phi_dev)
    phi_train, y_train, phi_dev, y_dev=phi_train.to_numpy(), y_train.to_numpy(), phi_dev.to_numpy(), y_dev.to_numpy()
    return phi_train, y_train, phi_dev, y_dev

def get_test_features(file_path):
    """Read test data, perform required preproccessing / augmentation
       and return final feature matrix.

    Args:
        file_path (str): path to test.csv

    Returns:
        phi_test: matrix of size (m,n) where m is number of test instances
                  and n is the dimension of the feature space.
    """
    df = pd.read_csv(file_path)
    phi_test=pd.dataframe(df,columns=['type','fixed acidity','volatile acidity','residual sulphur','chlorides','free sulfur','total sulfur','density','pH','sulpahtes','alcohol'])
    phi_test = scaler.fit_transform(phi_test)
    phi_test=phi_test.to_numpy()
    return phi_test

def compute_RMSE(phi, w , y) :
  error=((np.sum((np.square(np.dot(phi,w)-y))))**0.5)/y.shape[0]   
  return error

def generate_output(phi_test, w):
   output=pd.DataFrame(np.dot(phi_test,w),columns=output)
   output.to_csv('output.csv', header=[output])
   pass

def closed_soln(phi, y):
    """Function returns the solution w for Xw = y."""
    return np.linalg.pinv(phi).dot(y)

def gradient_descent(phi, y, phi_dev, y_dev) :
   # Implement gradient_descent using Mean Squared Error Loss
   # You may choose to use the dev set to determine point of convergence
    alpha=0.1
    m=np.shape(phi)
    n=np.shape(phi_dev)
    w=np.zeros([m[1]+1,1])
     
    phi=np.c_[np.ones(m[0]),phi]
    phi_dev=np.c_[np.ones(n[0]),phi_dev] 
    while compute_RMSE(phi_dev, w , y_dev)>31:
        
        error=compute_RMSE(phi, w , y)
        error1=np.dot(phi*w)-y
        delta=np.dot(np.transpose(phi),error1)/((error)*2)
        w=w-(alpha/m[0])*delta
    return w

def sgd(phi, y, phi_dev, y_dev) :
    m=np.shape(phi)
    n=np.shape(phi_dev)
     
    phi=np.c_[np.ones(m[0]),phi]
    phi_dev=np.c_[np.ones(n[0]),phi_dev] 
    
    w = np.random.randn(1,m[1]+1)  # Randomly initializing weights
    b = np.random.randn(1,1)   # Random intercept value
    
    epoch=1
    n_epochs=1000
    
    while epoch <= n_epochs:
        
        temp = phi.sample(k)

        X_tr = temp.iloc[:,0:13].values
        y_tr = temp.iloc[:,-1].values
        
        Lw = w
        
        loss = 0
        y_pred = []
        sq_loss = []
        
        for i in range(k):
              
            Lw = (-2/k * X_tr[i]) * (y_tr[i] - np.dot(X_tr[i],w.T) - b)
            
            w = w - learning_rate * Lw
            
            y_predicted = np.dot(X_tr[i],w.T)
            y_pred.append(y_predicted)
        
        loss = compute_RMSE(y_pred, y_tr)
            
        print("Epoch: %d, Loss: %.3f" %(epoch, loss))
        epoch+=1
        learning_rate = learning_rate/1.02
        
    return w

def pnorm(phi, y, phi_dev, y_dev, p) :
   # Implement gradient_descent with p-norm regularization using Mean Squared Error Loss
   # You may choose to use the dev set to determine point of convergence

    return w

def main():
    """ 
    The following steps will be run in sequence by the autograder.
    """
    ######## Task 2 #########
    phi, y, phi_dev, y_dev = get_labeled_features('train.csv')
    w1 = closed_soln(phi, y)
    w2 = gradient_descent(phi, y, phi_dev, y_dev)
    r1 = compute_RMSE(phi_dev, w1, y_dev)
    r2 = compute_RMSE(phi_dev, w2, y_dev)
    print('1a: ')
    print(abs(r1-r2))
    w3 = sgd(phi, y, phi_dev, y_dev)
    r3 = compute_RMSE(phi_dev, w3, y_dev)
    print('1c: ')
    print(abs(r2-r3))

    ######## Task 3 #########
    w_p2 = pnorm(phi, y, phi_dev, y_dev, 2)  
    w_p4 = pnorm(phi, y, phi_dev, y_dev, 4)  
    r_p2 = compute_RMSE(phi_dev, w_p2, y_dev)
    r_p4 = compute_RMSE(phi_dev, w_p4, y_dev)
    print('2: pnorm2')
    print(r_p2)
    print('2: pnorm4')
    print(r_p4)

    ######## Task 6 #########
    
    # Add code to run your selected method here
    # print RMSE on dev set with this method

main()

